ifdef::env-github,env-browser[:outfilesuffix: .adoc]
:rootdir: .
:imagesdir: {rootdir}/images
:toclevels: 2
:toc:
:numbered:
endif::[]

= Performance of Node.js Streams
:toc:

Note: For reproducing the tests, see xref:reproducing[reproducing].

== About Node.js Streams
Like unix pipes, Node streams are great for streaming chunks of data between
readers and writers to create arbitrary data processing pipelines.

Streams normally operate on strings and `Buffer` (or `Uint8Array`) objects.
In `object mode`, streams can also operate on arbitrary javascript values.

=== Buffering
Streams buffer incoming/outgoing data depending on the `highWaterMark` option.
For normal streams, the `highWaterMark` specifies a total number of bytes. For
object mode streams, the `highWaterMark` species a total number of objects.

When the size of the internal buffer reaches the `highWaterMark` threshold, the
stream will temporarily pause reading/writing data until that data is consumed.

A key goal of the stream API is to limit the buffering of data to acceptable
levels such that sources and destinations of differing speeds will not overwhelm
the available memory.

=== Encoding
Specifying an encoding causes the stream data to be returned as strings of the
specified encoding rather than as `Buffer` objects. By default, encoding is unspecified.

== Analyzing Stream Performance
We're interested in the overhead that come with using streams and specifically
the impact of chunk size, high water mark, encoding and object mode.

Our base case is a trivially simple pipeline consisting of a reader that
pushes the same chunk of memory (a memory reader) to a writer which simply discards
them (a null writer). By reusing memory, the memory reader avoids any memory
allocation overhead.

In more advanced cases, we look at the impact of adding intermediate transform
nodes. We also look at the performance of the Node.js file streaming reader and writer.

== The Base Case: Memory Reader to Null Writer

A `memory reader` is configured to iteratively push a chunk of data `null writer`.

image:https://raw.githubusercontent.com/venkatperi/stream-benchmarks/master/img/mem-to-null.png[mem
to null]

We use the following settings:

* `encoding`: Chunks are encoded as `buffers` to avoid conversion overhead.
* `highWaterMark`:
  ** `default`: Use the default `highWaterMark` (16KiB)
  ** `low`: Force the `highWaterMark` to be always lower than the chunk size
  ** `high`: Force the `highWaterMark` to be always higher than the chunk size
* `chunk size`: Ranges from 2KiB to 512KiB in steps of powers of 2.

The test runs in two modes:

* Fixed iterations: The total data through the pipeline will vary.
* Fixed size: More iterations when for smaller chunk sizes.

We use `benchmark.js` to determine the time taken for each of the above settings.

=== Fixed Number of Iterations

Reproduce with:

[source,bash]
----
$ bench-runner -g "iter.*(default|high|low).*buffer"
----

In this test, we push chunks 2KiB and larger through the pipeline, for a
fixed number of iterations (e.g. 200).

image:https://plot.ly/~venkatperi/48.png?share_key=rAm4c6kFTpKAsLeNElJFYA[image]

Here's what we can see from the graph (_higher is better_):

* Since the pipeline isn't processing buffers, chunk size doesn't have
an impact on performance.

* Chunk Size vs `highWaterMark`
 ** Performance is best when `chunk size < highWaterMark` (`high` plot).
 ** Likewise, performance is always poor when `chunk size > highWaterMark` 
 ** In the case of the default `highWaterMark`, note the drop in performance
when chunk size rises above 16KiB.

* Operating the stream in object mode boosts performance marginally.

=== Fixed Total Data Size

To reproduce:

[source,bash]
----
$ bench-runner -g "size.*(default|high|low).*false.*buffer"
----

image:https://plot.ly/~venkatperi/42.png?share_key=awtG8lMNLpAIYNFjVJtAvC%22[Image]

Here's what we can see in the above graph (_higher is better_): *
Performance is best when `chunk size < highWaterMark` (`high` plot).
Likewise, chunk sizes larger than the `highWaterMark` result in lower
performance (`low`). Note that the `default` plot has an inflection
point around the default `highWaterMark` of 16KiB.

[[thoughts]]
Thoughts
^^^^^^^^

It would seem that the hit in performance is a result of the `streams`
API trying to play nice and providing downstream components with the
number of bytes they requested. Looking under the hood, the following
snippet from
https://github.com/nodejs/readable-stream/blob/master/lib/_stream_readable.js[_stream_readable.js]
is responsible the decision to provide a `slice` (fast, for when
`highWaterMark` > chunk) or concatenate multiple buffers (slow, for when
`highWaterMark` < chunk):

[source,javascript]
----
// Extracts only enough buffered data to satisfy the amount requested.
// This function is designed to be inlinable, so please take care when making
// changes to the function body.
function fromListPartial(n, list, hasStrings) {
  var ret;
  if (n < list.head.data.length) {
    // slice is the same for buffers and strings
    ret = list.head.data.slice(0, n);
    list.head.data = list.head.data.slice(n);
  } else if (n === list.head.data.length) {
    // first chunk is a perfect match
    ret = list.shift();
  } else {
    // result spans more than one buffer
    ret = hasStrings ? copyFromBufferString(n, list) : copyFromBuffer(n, list);
  }
  return ret;
}
----

include::memory_allocation.adoc[]

== About the Tests

=== Reproducing Tests

To reproduce the tests:

Install `bench-runner`

[source,bash]
----
$ npm install -g bench-runner
----

Clone this repository

[source,bash]
----
$ git clone https://github.com/venkatperi/stream-benchmarks
$ cd stream-benchmarks
$ bench-runner <options>
----

=== benchmark.js
We use http://www.benchmarkjs.com[`benchmark.js`] with a mocha-like
runner https://www.npmjs.com/package/bench-runner[`bench-runner`].

=== Null Writer
The `null writer` is a `Writable` stream which accepts chunks and does
nothing with them.

[source,javascript]
----
NullWriter.prototype._write = function ( chunk, enc, cb ) {
  if ( chunk ) {
    this.count += chunk.length;
  }
  if ( this.delay ) {
    return setTimeout( cb, this.delay );
  }
  cb();
};
----

=== Memory Reader
The `memory reader` is a `Readable` stream which uses a
generator/iterator to push chunks of memory resident data. The
generators used here return the same memory chunk for every call to
`next()` to avoid the overhead of allocating memory.

[source,javascript]
----
function next( stream ) {
  var next = stream._generator.next();
  return stream.push( next.done ? null : next.value );
}

GeneratorReader.prototype._read = function ( n ) {
  while ( next( this ) ) {}
};
----
