:toc:

[[benchmarking-nodejs-streams]]
benchmarking nodejs streams
---------------------------


[[reproducing-tests]]
Reproducing Tests
^^^^^^^^^^^^^^^^^

To reproduce the tests:

Install `bench-runner`

[source,bash]
----
$ npm install -g bench-runner
----

Clone this repository

[source,bash]
----
$ git clone https://github.com/venkatperi/stream-benchmarks
$ cd stream-benchmarks
$ bench-runner <options>
----

[[chunk-size-and-the-high-water-mark]]
Chunk Size and the High Water Mark
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We want to see the effect of chunk size on streams operating in regular
(not `objectMode`) mode. A `memory reader` is configured to push chunks
of data `null writer`. Chunks are encoded as `buffers` to avoid any
conversion overhead. Chunk sizes start at 2KiB.

image:https://raw.githubusercontent.com/venkatperi/stream-benchmarks/master/img/mem-to-null.png[mem
to null]

We vary the `highWaterMark` to see it's effect on stream performance
(relevant when `objectMode` is false): * `default`: Use the default
`highWaterMark` (16KiB) * `low`: Force the `highWaterMark` to be always
lower than the chunk size * `high`: Force the `highWaterMark` to be
always higher than the chunk size

We run the tests in a couple of ways: * Fixed Iterations: The total data
through the pipeline will vary. * Fixed Size: More iterations when for
smaller chunk sizes.

[[fixed-number-of-iterations]]
Fixed Number of Iterations
^^^^^^^^^^^^^^^^^^^^^^^^^^

Reproduce with:

[source,bash]
----
$ bench-runner -g "iter.*(default|high|low).*buffer"
----

In this test, we push chunks 2KiB and larger through the pipeline, for a
fixed number of iterations (e.g. 200).

image:https://plot.ly/~venkatperi/48.png?share_key=rAm4c6kFTpKAsLeNElJFYA[image]

[[fixed-total-data-size]]
Fixed Total Data Size
^^^^^^^^^^^^^^^^^^^^^

To reproduce:

[source,bash]
----
$ bench-runner -g "size.*(default|high|low).*false.*buffer"
----

image:https://plot.ly/~venkatperi/42.png?share_key=awtG8lMNLpAIYNFjVJtAvC%22[Image]

Here's what we can see in the above graph (_higher is better_): *
Performance is best when `chunk size < highWaterMark` (`high` plot).
Likewise, chunk sizes larger than the `highWaterMark` result in lower
performance (`low`). Note that the `default` plot has an inflection
point around the default `highWaterMark` of 16KiB.

[[thoughts]]
Thoughts
^^^^^^^^

It would seem that the hit in performance is a result of the `streams`
API trying to play nice and providing downstream components with the
number of bytes they requested. Looking under the hood, the following
snippet from
https://github.com/nodejs/readable-stream/blob/master/lib/_stream_readable.js[_stream_readable.js]
is responsible the decision to provide a `slice` (fast, for when
`highWaterMark` > chunk) or concatenate multiple buffers (slow, for when
`highWaterMark` < chunk):

[source,javascript]
----
// Extracts only enough buffered data to satisfy the amount requested.
// This function is designed to be inlinable, so please take care when making
// changes to the function body.
function fromListPartial(n, list, hasStrings) {
  var ret;
  if (n < list.head.data.length) {
    // slice is the same for buffers and strings
    ret = list.head.data.slice(0, n);
    list.head.data = list.head.data.slice(n);
  } else if (n === list.head.data.length) {
    // first chunk is a perfect match
    ret = list.shift();
  } else {
    // result spans more than one buffer
    ret = hasStrings ? copyFromBufferString(n, list) : copyFromBuffer(n, list);
  }
  return ret;
}
----

[[about-the-tests]]
About the Tests
~~~~~~~~~~~~~~~

[[benchmark.js]]
benchmark.js
^^^^^^^^^^^^

We use http://www.benchmarkjs.com[`benchmark.js`] with a mocha-like
runner https://www.npmjs.com/package/bench-runner[`bench-runner`].

[[null-writer]]
Null Writer
^^^^^^^^^^^

The `null writer` is a `Writable` stream which accepts chunks and does
nothing with them.

[source,javascript]
----
NullWriter.prototype._write = function ( chunk, enc, cb ) {
  if ( chunk ) {
    this.count += chunk.length;
  }
  if ( this.delay ) {
    return setTimeout( cb, this.delay );
  }
  cb();
};
----

[[memory-reader]]
Memory Reader
^^^^^^^^^^^^^

The `memory reader` is a `Readable` stream which uses a
generator/iterator to push chunks of memory resident data. The
generators used here return the same memory chunk for every call to
`next()` to avoid the overhead of allocating memory.

[source,javascript]
----
function next( stream ) {
  var next = stream._generator.next();
  return stream.push( next.done ? null : next.value );
}

GeneratorReader.prototype._read = function ( n ) {
  while ( next( this ) ) {}
};
----
