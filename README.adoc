ifdef::env-github,env-browser[:outfilesuffix: .adoc]
:rootdir: .
:imagesdir: {rootdir}/images
:toclevels: 2
:toc:
:numbered:
endif::[]

= Performance of Node.js Streams
:toc:

NOTE: For reproducing the tests, see link:doc/reproducing.adoc[Reproducing the Tests].

== Analyzing Stream Performance
We're interested in the overhead that come with using streams and specifically
the impact of chunk size, high water mark, encoding and object mode.

Our base case is a trivially simple pipeline consisting of a reader that
pushes the same chunk of memory (a memory reader) to a writer which simply discards
them (a null writer). By reusing memory, the memory reader avoids any memory
allocation overhead.

In more advanced cases, we look at the impact of adding intermediate transform
nodes. We also look at the performance of the Node.js file streaming reader and writer.

== The Base Case: Memory Reader to Null Writer

A `memory reader` is configured to iteratively push a chunk of data `null writer`.

image:https://raw.githubusercontent.com/venkatperi/stream-benchmarks/master/img/mem-to-null.png[mem
to null]

We use the following settings:

* `encoding`: Chunks are encoded as `buffers` to avoid conversion overhead.
* `highWaterMark`:
  ** `default`: Use the default `highWaterMark` (16KiB)
  ** `low`: Force the `highWaterMark` to be always lower than the chunk size
  ** `high`: Force the `highWaterMark` to be always higher than the chunk size
* `chunk size`: Ranges from 2KiB to 512KiB in steps of powers of 2.

The test runs in two modes:

* Fixed iterations: The total data through the pipeline will vary.
* Fixed size: More iterations when for smaller chunk sizes.

We use `benchmark.js` to determine the time taken for each of the above settings.

=== Fixed Number of Iterations

Reproduce with:

[source,bash]
----
$ bench-runner -g "iter.*(default|high|low).*buffer"
----

In this test, we push chunks 2KiB and larger through the pipeline, for a
fixed number of iterations (e.g. 200).

image:https://plot.ly/~venkatperi/48.png?share_key=rAm4c6kFTpKAsLeNElJFYA[image]

Here's what we can see from the graph (_higher is better_):

* Since the pipeline isn't processing buffers, chunk size doesn't have
an impact on performance.

* Chunk Size vs `highWaterMark`
 ** Performance is best when `chunk size < highWaterMark` (`high` plot).
 ** Likewise, performance is always poor when `chunk size > highWaterMark`
 ** In the case of the default `highWaterMark`, note the drop in performance
when chunk size rises above 16KiB.

* Operating the stream in object mode boosts performance marginally.

=== Fixed Total Data Size

To reproduce:

[source,bash]
----
$ bench-runner -g "size.*(default|high|low).*false.*buffer"
----

image:https://plot.ly/~venkatperi/42.png?share_key=awtG8lMNLpAIYNFjVJtAvC%22[Image]

Here's what we can see in the above graph (_higher is better_): *
Performance is best when `chunk size < highWaterMark` (`high` plot).
Likewise, chunk sizes larger than the `highWaterMark` result in lower
performance (`low`). Note that the `default` plot has an inflection
point around the default `highWaterMark` of 16KiB.

[[thoughts]]
Thoughts
^^^^^^^^

It would seem that the hit in performance is a result of the `streams`
API trying to play nice and providing downstream components with the
number of bytes they requested. Looking under the hood, the following
snippet from
https://github.com/nodejs/readable-stream/blob/master/lib/_stream_readable.js[_stream_readable.js]
is responsible the decision to provide a `slice` (fast, for when
`highWaterMark` > chunk) or concatenate multiple buffers (slow, for when
`highWaterMark` < chunk):

[source,javascript]
----
// Extracts only enough buffered data to satisfy the amount requested.
// This function is designed to be inlinable, so please take care when making
// changes to the function body.
function fromListPartial(n, list, hasStrings) {
  var ret;
  if (n < list.head.data.length) {
    // slice is the same for buffers and strings
    ret = list.head.data.slice(0, n);
    list.head.data = list.head.data.slice(n);
  } else if (n === list.head.data.length) {
    // first chunk is a perfect match
    ret = list.shift();
  } else {
    // result spans more than one buffer
    ret = hasStrings ? copyFromBufferString(n, list) : copyFromBuffer(n, list);
  }
  return ret;
}
----


== About the Tests


=== benchmark.js
We use http://www.benchmarkjs.com[`benchmark.js`] with a mocha-like
runner https://www.npmjs.com/package/bench-runner[`bench-runner`].
